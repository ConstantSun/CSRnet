{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "hi\n"
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/hang/.cache/torch/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [01:33<00:00, 5.89MB/s]\n"
    }
   ],
   "source": [
    "vgg = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_dict = { k: v for k,v in  vgg.state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "features.0.weight\nfeatures.0.bias\nfeatures.2.weight\nfeatures.2.bias\nfeatures.5.weight\nfeatures.5.bias\nfeatures.7.weight\nfeatures.7.bias\nfeatures.10.weight\nfeatures.10.bias\nfeatures.12.weight\nfeatures.12.bias\nfeatures.14.weight\nfeatures.14.bias\nfeatures.17.weight\nfeatures.17.bias\nfeatures.19.weight\nfeatures.19.bias\nfeatures.21.weight\nfeatures.21.bias\nfeatures.24.weight\nfeatures.24.bias\nfeatures.26.weight\nfeatures.26.bias\nfeatures.28.weight\nfeatures.28.bias\nclassifier.0.weight\nclassifier.0.bias\nclassifier.3.weight\nclassifier.3.bias\nclassifier.6.weight\nclassifier.6.bias\n"
    }
   ],
   "source": [
    "for k in vgg_dict:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import models\n",
    "from utils import save_net,load_net\n",
    "\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self, load_weights=False):\n",
    "        super(CSRNet, self).__init__()\n",
    "        self.seen = 0\n",
    "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
    "        self.backend_feat  = [512, 512, 512,256,128,64]\n",
    "        self.frontend = make_layers(self.frontend_feat)\n",
    "        self.backend = make_layers(self.backend_feat,in_channels = 512,dilation = True)\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        # if not load_weights:\n",
    "        #     mod = models.vgg16(pretrained = True)\n",
    "        #     self._initialize_weights()\n",
    "        #     for i in range(len(self.frontend.state_dict().items())):\n",
    "        #         self.frontend.state_dict().items()[i][1].data[:] = mod.state_dict().items()[i][1].data[:]\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "                \n",
    "def make_layers(cfg, in_channels = 3,batch_norm=False,dilation = False):\n",
    "    if dilation:\n",
    "        d_rate = 2\n",
    "    else:\n",
    "        d_rate = 1\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate,dilation = d_rate)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CSRNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem = {k:v for k, v in model.frontend.state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.weight\n0.bias\n2.weight\n2.bias\n5.weight\n5.bias\n7.weight\n7.bias\n10.weight\n10.bias\n12.weight\n12.bias\n14.weight\n14.bias\n17.weight\n17.bias\n19.weight\n19.bias\n21.weight\n21.bias\n"
    }
   ],
   "source": [
    "for k in tem:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[[ 0.0914, -0.1180,  0.1049],\n          [ 0.1479, -0.0080, -0.0952],\n          [-0.1904, -0.1134, -0.1803]],\n\n         [[ 0.1744,  0.1923,  0.1741],\n          [ 0.1563, -0.0596, -0.0509],\n          [-0.1682,  0.1444,  0.1384]],\n\n         [[-0.0643, -0.1868,  0.0416],\n          [-0.1189, -0.0877, -0.1616],\n          [ 0.0969, -0.0021,  0.0601]]],\n\n\n        [[[ 0.1483,  0.1916,  0.0682],\n          [ 0.1739, -0.0262,  0.0132],\n          [ 0.0548,  0.1791, -0.1157]],\n\n         [[ 0.0254,  0.1337,  0.0600],\n          [ 0.0471,  0.0198, -0.1535],\n          [ 0.1668,  0.1908, -0.1569]],\n\n         [[ 0.0415,  0.1057, -0.0806],\n          [-0.1021,  0.1550, -0.1712],\n          [-0.0975,  0.0737, -0.0408]]],\n\n\n        [[[-0.1321,  0.0401, -0.0989],\n          [ 0.0663,  0.0818,  0.0561],\n          [ 0.1557, -0.1012,  0.0614]],\n\n         [[-0.0418, -0.0063, -0.0104],\n          [-0.1732,  0.0542,  0.0457],\n          [ 0.0782,  0.1920, -0.1191]],\n\n         [[-0.0204,  0.1866,  0.0210],\n          [-0.1835,  0.1052, -0.1609],\n          [ 0.1914,  0.0758, -0.1612]]],\n\n\n        ...,\n\n\n        [[[ 0.0201, -0.0178, -0.0977],\n          [ 0.0408,  0.1291,  0.0007],\n          [-0.1093, -0.0202, -0.0507]],\n\n         [[-0.0725,  0.0170, -0.0117],\n          [-0.1327,  0.1836, -0.0113],\n          [-0.0392, -0.1911,  0.0253]],\n\n         [[ 0.0038,  0.1346, -0.0888],\n          [-0.1191,  0.0884,  0.1176],\n          [ 0.1073, -0.1822, -0.1609]]],\n\n\n        [[[-0.0543,  0.1422, -0.0246],\n          [-0.0897, -0.0474,  0.1253],\n          [ 0.1462,  0.0466, -0.0747]],\n\n         [[-0.0351, -0.0293,  0.0318],\n          [-0.0510, -0.1534, -0.0978],\n          [-0.1641, -0.0480, -0.0996]],\n\n         [[-0.0635, -0.0555,  0.0045],\n          [-0.1452,  0.1384,  0.1385],\n          [-0.1201, -0.1882, -0.0516]]],\n\n\n        [[[-0.1495,  0.0064,  0.0313],\n          [-0.0073,  0.0811, -0.0626],\n          [-0.0824, -0.0294,  0.1503]],\n\n         [[ 0.0601, -0.1099,  0.0622],\n          [ 0.1588, -0.1902, -0.1196],\n          [ 0.0074,  0.0897,  0.0550]],\n\n         [[ 0.0757,  0.0347,  0.0695],\n          [-0.0661, -0.1156, -0.0591],\n          [ 0.1829,  0.0274, -0.0888]]]])"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "tem['0.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(-2.4552)"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "torch.sum(tem['0.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([64])"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "tem['0.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([64, 64, 3, 3])"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "tem['2.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595219438146",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}